{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "791010b2",
   "metadata": {},
   "source": [
    "Azure AI Inference Script using GitHub Models\n",
    "Free to use in GitHub Codespaces with your GitHub token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f982cc5e",
   "metadata": {},
   "source": [
    "# Check if token and good endpoibt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73a73358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token type: Classic\n",
      "Status: 200\n",
      "‚úÖ √áa marche!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "token = os.environ.get(\"GITHUB_TOKEN\")\n",
    "print(f\"Token type: {'Classic' if token.startswith('ghp_') else 'Fine-grained'}\")\n",
    "\n",
    "response = requests.post(\n",
    "    \"https://models.inference.ai.azure.com/chat/completions\",\n",
    "    headers={\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"},\n",
    "    json={\"model\": \"gpt-4o-mini\", \"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}]}\n",
    ")\n",
    "\n",
    "print(f\"Status: {response.status_code}\")\n",
    "if response.status_code == 200:\n",
    "    print(\"‚úÖ √áa marche!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "030e1c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ü§ñ GITHUB MODELS AI CHAT\n",
      "============================================================\n",
      "\n",
      "Mod√®les disponibles:\n",
      "  ‚Ä¢ gpt-4o-mini (recommand√© - plus de requ√™tes)\n",
      "  ‚Ä¢ gpt-4o (puissant mais limit√©)\n",
      "  ‚Ä¢ Phi-3-medium-128k-instruct\n",
      "  ‚Ä¢ Llama-3.2-90B-Vision-Instruct\n",
      "\n",
      "üí° Astuce: Utilisez Ctrl+C √† tout moment pour quitter\n",
      "------------------------------------------------------------\n",
      "\n",
      "Mod√®le s√©lectionn√©: gpt-4o-mini\n",
      "\n",
      "============================================================\n",
      "üìù Test rapide\n",
      "============================================================\n",
      "Prompt: Complete this text and explain what it is: 'oh say can you see'\n",
      "\n",
      "ü§î R√©flexion en cours...\n",
      "\n",
      "ü§ñ R√©ponse: The text \"Oh say can you see\" is the opening line of the national anthem of the United States, titled \"The Star-Spangled Banner.\" The song was written by Francis Scott Key in 1814. Key was inspired to write the poem, which later became the anthem, after witnessing the bombardment of Fort McHenry during the War of 1812 and seeing the American flag still flying over the fort the next morning.\n",
      "\n",
      "The anthem reflects themes of patriotism, resilience, and the struggle for freedom. It is often performed at public events, especially sporting events, and is a symbol of American national pride. The full anthem consists of four stanzas, but typically only the first stanza is sung at events. \n",
      "\n",
      "If you would like me to provide more details about the anthem or its history, feel free to ask!\n",
      "\n",
      "============================================================\n",
      "üí¨ MODE INTERACTIF\n",
      "============================================================\n",
      "‚Ä¢ Tapez votre message et appuyez sur Entr√©e\n",
      "‚Ä¢ Tapez 'quit', 'exit' ou 'q' pour quitter\n",
      "‚Ä¢ Utilisez Ctrl+C pour interrompre √† tout moment\n",
      "------------------------------------------------------------\n",
      "\n",
      "ü§ñ IA: La capitale de la France est Paris.\n",
      "\n",
      "üëã Au revoir!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Azure AI Inference Script avec gestion correcte des interruptions\n",
    "Utilisez Ctrl+C pour interrompre √† tout moment\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import signal\n",
    "import time\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "def signal_handler(signum, frame):\n",
    "    \"\"\"Gestionnaire pour Ctrl+C\"\"\"\n",
    "    print(\"\\n\\nüëã Interruption d√©tect√©e. Au revoir!\")\n",
    "    sys.exit(0)\n",
    "\n",
    "def setup_client():\n",
    "    \"\"\"Setup the Azure AI client with GitHub token\"\"\"\n",
    "    # Enregistrer le gestionnaire de signal\n",
    "    signal.signal(signal.SIGINT, signal_handler)\n",
    "    \n",
    "    # Try to get token from environment variable\n",
    "    token = os.environ.get(\"GITHUB_TOKEN\")\n",
    "    \n",
    "    if not token:\n",
    "        # If not in env, try to get it from Codespace secrets\n",
    "        token = os.environ.get(\"CODESPACES_SECRET_GITHUB_TOKEN\")\n",
    "    \n",
    "    if not token:\n",
    "        print(\"ERROR: GITHUB_TOKEN not found!\")\n",
    "        print(\"Please set it using one of these methods:\")\n",
    "        print(\"1. In Codespaces: Settings > Secrets > New secret\")\n",
    "        print(\"2. Or run: export GITHUB_TOKEN='your-token-here'\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    endpoint = \"https://models.inference.ai.azure.com\"\n",
    "    \n",
    "    client = ChatCompletionsClient(\n",
    "        endpoint=endpoint,\n",
    "        credential=AzureKeyCredential(token),\n",
    "    )\n",
    "    \n",
    "    return client\n",
    "\n",
    "def get_completion(prompt: str,\n",
    "                   client, \n",
    "                   model_name=\"gpt-4o-mini\", \n",
    "                   temperature=1.0, \n",
    "                   max_tokens=1000, \n",
    "                   top_p=1.0,\n",
    "                   system_prompt: str = \"You are a helpful assistant.\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Get completion from the AI model\n",
    "    Note: Using gpt-4o-mini by default as it's more cost-effective\n",
    "    Generate a text completion using an OpenAI-compatible client.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): User input or question.\n",
    "        client: OpenAI client (e.g., openai.OpenAI()).\n",
    "        model_name (str): Model name to use (default: gpt-4o-mini).\n",
    "        temperature (float): Controls creativity (0.0‚Äì2.0).\n",
    "        max_tokens (int): Maximum number of tokens to generate.\n",
    "        top_p (float): Nucleus sampling parameter (0.0‚Äì1.0).\n",
    "        system_prompt (str): Optional system instruction.\n",
    "\n",
    "    Returns:\n",
    "        str: The model's text response.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.complete(\n",
    "            messages=[\n",
    "                SystemMessage(content=system_prompt),\n",
    "                UserMessage(content=prompt),\n",
    "            ],\n",
    "            model=model_name,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            top_p=top_p\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except KeyboardInterrupt:\n",
    "        raise  # Re-raise pour que le main puisse le g√©rer\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "    \n",
    "def main():\n",
    "    \"\"\"Main execution function avec gestion des interruptions\"\"\"\n",
    "    try:\n",
    "        # Setup client\n",
    "        client = setup_client()\n",
    "        \n",
    "        # Available models in GitHub Models (free tier)\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ü§ñ GITHUB MODELS AI CHAT\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"\\nMod√®les disponibles:\")\n",
    "        print(\"  ‚Ä¢ gpt-4o-mini (recommand√© - plus de requ√™tes)\")\n",
    "        print(\"  ‚Ä¢ gpt-4o (puissant mais limit√©)\")\n",
    "        print(\"  ‚Ä¢ Phi-3-medium-128k-instruct\")\n",
    "        print(\"  ‚Ä¢ Llama-3.2-90B-Vision-Instruct\")\n",
    "        print(\"\\nüí° Astuce: Utilisez Ctrl+C √† tout moment pour quitter\")\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        # Choose model\n",
    "        model_name = \"gpt-4o-mini\"\n",
    "        print(f\"\\nMod√®le s√©lectionn√©: {model_name}\")\n",
    "        \n",
    "        # Example 1: Simple completion\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üìù Test rapide\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        text = \"oh say can you see\"\n",
    "        prompt = f\"Complete this text and explain what it is: '{text}'\"\n",
    "        \n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(\"\\nü§î R√©flexion en cours...\")\n",
    "        \n",
    "        response = get_completion(prompt, client, model_name)\n",
    "        print(f\"\\nü§ñ R√©ponse: {response}\")\n",
    "        \n",
    "        # Interactive mode\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üí¨ MODE INTERACTIF\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"‚Ä¢ Tapez votre message et appuyez sur Entr√©e\")\n",
    "        print(\"‚Ä¢ Tapez 'quit', 'exit' ou 'q' pour quitter\")\n",
    "        print(\"‚Ä¢ Utilisez Ctrl+C pour interrompre √† tout moment\")\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                # Prompt avec gestion d'interruption\n",
    "                user_input = input(\"\\nüí≠ Vous: (use q for exit)\")\n",
    "                \n",
    "                # V√©rifier les commandes de sortie\n",
    "                if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "                    print(\"\\nüëã Au revoir!\")\n",
    "                    break\n",
    "                \n",
    "                # V√©rifier si l'entr√©e est vide\n",
    "                if not user_input.strip():\n",
    "                    print(\"‚ö†Ô∏è  Veuillez entrer un message.\")\n",
    "                    continue\n",
    "                \n",
    "                # Indicateur de traitement\n",
    "                print(\"\\nü§ñ IA: \", end=\"\", flush=True)\n",
    "                print(\"(en cours...)\", end=\"\", flush=True)\n",
    "                \n",
    "                # Obtenir la r√©ponse\n",
    "                response = get_completion(user_input, client, model_name)\n",
    "                \n",
    "                # Effacer \"(en cours...)\" et afficher la r√©ponse\n",
    "                print(\"\\rü§ñ IA: \" + \" \" * 20, end=\"\\r\")  # Effacer la ligne\n",
    "                print(f\"ü§ñ IA: {response}\")\n",
    "                \n",
    "            except EOFError:\n",
    "                # Gestion de Ctrl+D (fin de fichier)\n",
    "                print(\"\\n\\nüëã Fin de session d√©tect√©e. Au revoir!\")\n",
    "                break\n",
    "            except KeyboardInterrupt:\n",
    "                # Gestion de Ctrl+C pendant l'input\n",
    "                print(\"\\n\\n‚ö†Ô∏è  Interruption d√©tect√©e.\")\n",
    "                confirm = input(\"Voulez-vous vraiment quitter? (o/n): \")\n",
    "                if confirm.lower() in ['o', 'oui', 'y', 'yes']:\n",
    "                    print(\"üëã Au revoir!\")\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"Reprise du chat...\")\n",
    "                    continue\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "        # Gestion globale de Ctrl+C\n",
    "        print(\"\\n\\nüëã Programme interrompu. Au revoir!\")\n",
    "        sys.exit(0)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n\\n‚ùå Erreur inattendue: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Protection suppl√©mentaire pour les interruptions\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nüëã Arr√™t du programme.\")\n",
    "        sys.exit(0)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Erreur fatale: {e}\")\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311609d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
